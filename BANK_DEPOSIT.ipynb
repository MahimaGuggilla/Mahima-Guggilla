{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sjGDggvSOyaO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import itertools"
      ],
      "metadata": {
        "id": "1aysG6MKO4Pq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zx3WMh4eO_EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train=pd.read_csv('/content/drive/MyDrive/week6/Classification Datsets/Classification Datsets/Bank Client Deposit Data set Classification/Bank Client Deposit Data set Classification.csv')"
      ],
      "metadata": {
        "id": "PJLeehSvPNUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.shape"
      ],
      "metadata": {
        "id": "r64KDCQvPRsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.head()"
      ],
      "metadata": {
        "id": "A2W3nBYTPX8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.columns"
      ],
      "metadata": {
        "id": "Ssiia6hVQXNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.isnull().sum()"
      ],
      "metadata": {
        "id": "9WxeCCsjQbQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.describe(include='all')"
      ],
      "metadata": {
        "id": "7EPqfpwcQeqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.dtypes"
      ],
      "metadata": {
        "id": "dl2FBqEEQofe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.info()"
      ],
      "metadata": {
        "id": "QROH9ErCQunB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.plot(kind='box',subplots=True,layout=(7,7),figsize=(30,30))\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "5e408yk7pwif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.plot(kind='box',subplots=True,layout=(7,7),figsize=(30,30))\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "BS1Zbm5tQzRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = (dataset_train.dtypes == 'object')\n",
        "object_cols = list(obj[obj].index)\n",
        "print(\"Categorical variables:\",len(object_cols))\n",
        " \n",
        "int_ = (dataset_train.dtypes == 'int')\n",
        "num_cols = list(int_[int_].index)\n",
        "print(\"Integer variables:\",len(num_cols))"
      ],
      "metadata": {
        "id": "9v14uAsERCt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(dataset_train.corr(), cmap=\"RdBu\",annot=True)\n",
        "plt.title(\"Correlations Between Variables\", size=15)\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "jU789oJdRGXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num=dataset_train._get_numeric_data() \n",
        "num.head()"
      ],
      "metadata": {
        "id": "mbMYI18sRKS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.nunique()"
      ],
      "metadata": {
        "id": "qb-1c0eeROWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2_cat=dataset_train.drop(num,axis=1)\n",
        "df2_cat.head()"
      ],
      "metadata": {
        "id": "p4txdhHPRRW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=dataset_train.drop(['age','education','day','campaign','pdays','job'], axis=1)\n",
        "df"
      ],
      "metadata": {
        "id": "urgRcFEJRVXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat=dataset_train.drop(num,axis=1)\n",
        "cat.head()"
      ],
      "metadata": {
        "id": "ZQhWLPNeTTfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train['marital'].unique() "
      ],
      "metadata": {
        "id": "zj6xB-exToxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train['default'].unique()"
      ],
      "metadata": {
        "id": "v5q0yWWAUfoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"y\"] = df[\"y\"].replace({\"no\": 0, \"yes\": 1})"
      ],
      "metadata": {
        "id": "FTcvPqMvUtyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert categorical columns into dummy variables\n",
        "cat_cols = ['marital','default','housing','loan','contact','month','poutcome']\n",
        "df_dummies = pd.get_dummies(dataset_train[cat_cols], drop_first=True)\n",
        "\n",
        "# Concatenate the dummy variables with the original dataframe\n",
        "df = pd.concat([df, df_dummies], axis=1)\n",
        "\n",
        "# Drop the original categorical columns\n",
        "df.drop(columns=cat_cols, inplace=True)\n",
        "\n",
        "# Print the updated dataframe\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "HBRRQYqmVzK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "sl_7J_8LWLC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class StandardScaler:\n",
        "    def __init__(self):\n",
        "        self.mean_ = None\n",
        "        self.std_ = None\n",
        "        \n",
        "    def fit(self, X):\n",
        "        self.mean_ = np.mean(X, axis=0)\n",
        "        self.std_ = np.std(X, axis=0)\n",
        "        \n",
        "    def transform(self, X):\n",
        "        return (X - self.mean_) / self.std_\n",
        "    \n",
        "    def fit_transform(self, X):\n",
        "        self.fit(X)\n",
        "        return self.transform(X)\n"
      ],
      "metadata": {
        "id": "y34iSTT3WOcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num = ['balance','duration']\n",
        "X_train_num = df[num]\n",
        "\n",
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training set\n",
        "scaler.fit(X_train_num)\n",
        "\n",
        "# Scale the numerical features in the training set\n",
        "df[num] = scaler.transform(X_train_num)\n"
      ],
      "metadata": {
        "id": "mhl-3u92WSDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the target variable from the input features\n",
        "y = df['y']\n",
        "X = df.drop('y', axis=1) #axis=1 the operation is performed along the columns"
      ],
      "metadata": {
        "id": "-AF8YslKWZr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "train_size = int(len(dataset_train) * 0.7) # 70% of the data is used for training\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Print the size of each set\n",
        "print('Training data:', len(X_train))\n",
        "print('test data:', len(X_test))"
      ],
      "metadata": {
        "id": "XdzRSALhWdDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "nagmros5XBU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "GEhzJJSxkeWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "metadata": {
        "id": "u6xJv7tJkkwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()\n"
      ],
      "metadata": {
        "id": "ASsILU0gWhNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, num_features):\n",
        "        self.weights1 = np.random.rand(6, num_features)\n",
        "        self.weights2 = np.random.rand(1, 6)\n",
        "  \n",
        "\n",
        "    def forward(self, X):\n",
        "        self.hidden_layer = sigmoid(np.dot(X, self.weights1.T))\n",
        "        self.output_layer = sigmoid(np.dot(self.hidden_layer, self.weights2.T))\n",
        "        return self.output_layer\n",
        "    \n",
        "    def train(self, X_train, y_train, lr, epochs):\n",
        "        y_train = y_train.values.reshape(-1, 1)  # convert to numpy array and reshape\n",
        "        for i in range(epochs):\n",
        "            # forward propagation\n",
        "            output = self.forward(X_train)\n",
        "\n",
        "            # calculate error\n",
        "            error = y_train - output\n",
        "\n",
        "            # backpropagation\n",
        "            output_derivative = output * (1 - output)\n",
        "            hidden_layer_derivative = self.hidden_layer * (1 - self.hidden_layer)\n",
        "            delta2 = error * output_derivative\n",
        "            delta1 = np.dot(delta2, self.weights2) * hidden_layer_derivative\n",
        "\n",
        "            # update weights\n",
        "            self.weights2 += lr * np.dot(delta2.T, self.hidden_layer)\n",
        "            self.weights1 += lr * np.dot(delta1.T, X_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "r4SNZWNyWkYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork(num_features=24)\n",
        "nn.train(X_train, y_train, lr=0.01, epochs=100)"
      ],
      "metadata": {
        "id": "KomDJ6FDWqzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = nn.forward(X_train)\n",
        "\n",
        "# convert predictions to binary (0 or 1) using a threshold value of 0.5\n",
        "y_pred_binary = np.where(y_pred >= 0.5, 1, 0)\n",
        "\n",
        "# compare predictions to actual labels and calculate accuracy\n",
        "accuracy = np.mean(y_pred_binary == y_train.values.reshape(-1, 1))\n",
        "print('Train accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "Vd_9EYUe_iIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_t = nn.forward(X_test)\n",
        "\n",
        "# convert predictions to binary (0 or 1) using a threshold value of 0.5\n",
        "y_pred_binary_t = np.where(y_pred_t >= 0.5, 1, 0)\n",
        "\n",
        "# compare predictions to actual labels and calculate accuracy\n",
        "accuracy = np.mean(y_pred_binary_t == y_test.values.reshape(-1, 1))\n",
        "print('Test accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "eC_5scn1_ju9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assume X_test and y_test are the test dataset and labels\n",
        "# predict labels using traine\n",
        "\n",
        "# create confusion matrix\n",
        "confusion_matrix = np.zeros((2, 2))\n",
        "for i in range(len(y_test)):\n",
        "    true_label = int(y_test.iloc[i])\n",
        "    predicted_label = int(y_pred_binary_t[i][0])\n",
        "    confusion_matrix[true_label][predicted_label] += 1\n",
        "\n",
        "# print confusion matrix\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix)\n"
      ],
      "metadata": {
        "id": "k3HbuHCP_nta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# define hyperparameters and their possible values to try out\n",
        "learning_rates = [0.1, 0.01, 0.001]\n",
        "epochs = [100,200,300]\n",
        "hidden_layer_sizes = [40, 45, 30]\n",
        "\n",
        "# create a list of all possible hyperparameter combinations\n",
        "hyperparameter_combinations = list(itertools.product(learning_rates, epochs, hidden_layer_sizes))\n",
        "\n",
        "# initialize variables to keep track of best hyperparameters and best accuracy\n",
        "best_hyperparameters = None\n",
        "best_accuracy = 0\n",
        "\n",
        "# loop over all possible hyperparameter combinations\n",
        "for lr, epoch, hidden_layer_size in hyperparameter_combinations:\n",
        "    # train model with current hyperparameters\n",
        "    nn = NeuralNetwork(num_features=24)\n",
        "    nn.train(X_train, y_train, lr=lr, epochs=epoch)\n",
        "    \n",
        "    # make predictions on train data\n",
        "    y_pred = nn.forward(X_train)\n",
        "    y_pred_binary = np.where(y_pred >= 0.5, 1, 0)\n",
        "\n",
        "    # calculate accuracy\n",
        "    accuracy = np.mean(y_pred_binary == y_train.values.reshape(-1, 1))\n",
        "    # make predictions on test data\n",
        "    y_pred = nn.forward(X_test)\n",
        "    y_pred_binary_t = np.where(y_pred >= 0.5, 1, 0)\n",
        "\n",
        "    # calculate accuracy\n",
        "    accuracy = np.mean(y_pred_binary_t == y_test.values.reshape(-1, 1))\n",
        "    \n",
        "\n",
        "    # if current accuracy is better than previous best, update best hyperparameters and best accuracy\n",
        "    if accuracy > best_accuracy:\n",
        "        best_hyperparameters = (lr, epoch, hidden_layer_size)\n",
        "        best_accuracy = accuracy\n",
        "    \n",
        "print(\"Best hyperparameters:\", best_hyperparameters)\n",
        "print(\"Best test accuracy:\", best_accuracy)\n",
        "print(\"Best train accuracy:\", best_accuracy)"
      ],
      "metadata": {
        "id": "U6iwr9uB_xYS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}